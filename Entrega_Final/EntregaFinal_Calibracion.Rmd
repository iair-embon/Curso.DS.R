---
title: "TrabajoFinal"
author: "Alejandro Ramos Usaj, Agostina Sacson y Iair Embon"
date: '2022-07-25'
output: html_document
---

```{r}
library(tidyverse)
library(glmnet)
```

Cargo los datos
```{r}
# voy a la carpeta del proyecto
root <- rprojroot::is_rstudio_project
basename(getwd())

# load the function to get the df list
load("./df_total_filtered.Rda")
```

Saco la calibracion para cada participante, ver Fleming y Lau 2014
```{r}
df_calibracion <- df_total %>%
  group_by(Participant, confidence_key) %>%
  summarise(prop_correcta = mean(discrimination_is_correct)) %>%
  rename(ConfidenceKey = confidence_key)
  
df_calibracion.calculada <- df_total %>% 
  pivot_longer(cols = starts_with("ConfKey")) %>%
  distinct(Participant, name, .keep_all = TRUE) %>% 
  select(Participant, name, value) %>% 
  mutate(ConfidenceKey = as.integer(str_extract(name, "\\d"))) %>%
  left_join(df_calibracion) %>%
  drop_na() %>%
  mutate(ConfidenceKey = (ConfidenceKey+2)/6) %>%
  group_by(Participant) %>%
  mutate(Conf.norm = value / sum(value)) %>%
  mutate( preCalibracion = value*(prop_correcta - ConfidenceKey)**2) %>%
  summarise(Calibracion  = mean(preCalibracion))

# exploro calibracion en base al promedio de confianza de cada persona y le ajusto una lineal con geom_smooth
ggplot(mapping = aes(df_calibracion.calculada$Calibracion, d$ConfMean))+
  geom_smooth(method = "lm") + geom_point()
```

Convierto el df por trials en un df por participantes
```{r}
d <- df_total %>%
  select(!c(RelyOn,
            Problems,
            ConfKey1,
            ConfKey2,
            ConfKey3,
            ConfKey4,
            discrimination_is_correct,
            confidence_key,
            trials,
            PointDifference,
            ReacTime_DiscTask,
            ReacTime_ConfTask)) %>%
  distinct(Participant,.keep_all = TRUE)
```

Exploro la confianza promedio de cada participante en base al desempeÃ±o en la tarea. Por el procedimiento en escalera deberian tener una precision (proporcion de respuestas correctas) del 0.72. 
```{r}
d %>%  ## no me estarian saliendo las leyendas, despues reviso
  select(ConfMean,PC,Participant) %>% 
  # normalizo el promedio de confianza por sujeto para que este en una escala similar a la del desempeno
  mutate(ConfMean = ConfMean/4, nr = row_number()) %>%
  arrange(ConfMean) %>%
  mutate(nr = row_number()) %>%
  ggplot(aes(x= nr)) +
  geom_point(aes(y=ConfMean), size = 2, color = "black") +
  geom_point(aes(y=PC), size = 2, color = "grey")+
  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
  scale_y_continuous(expand = expansion(mult = c(0.01, 0.1))) +
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        plot.margin = margin(1, 1,1, 1, "cm"),
        legend.text =  element_text(size = 25),
        legend.position = c(0.7, 0.2),
        legend.background = element_blank(),
        legend.key = element_blank(),
        legend.title = element_blank(),
        panel.background = element_blank(),
        axis.text.x = element_text(size = 30),
        axis.text.y = element_text(size = 30),
        axis.title.x = element_blank(),
        axis.title.y = element_blank())
```

Agregamos una variable que va a decir si es overconfidence y underconfidence para cada participante. 
1 si es overconfidence.
```{r}
d <- d %>%
  mutate(Calibracion = df_calibracion.calculada$Calibracion)

hist(d$Calibracion)
```

Corremos una regresion lineal para predecir calibracion con los dominios de los rasgos disfuncionales de la personalidad
```{r}
model <- lm(Calibracion ~ DomainAntagonism + 
               DomainDetachment +
               DomainDisinhibition +
               DomainNegativeAffect + 
               DomainPsychoticism,
             data=d)
summary(model)
```

Corro otro modelo lineal pero esta vez controlo por edad y genero
```{r}
model2 <- lm(Calibracion ~ DomainAntagonism + 
               DomainDetachment +
               DomainDisinhibition +
               DomainNegativeAffect + 
               DomainPsychoticism +
               gender +
               age,
             data=d)
summary(model2)
```

#Regresion lineal usando metodos de regularizaciones

Corro una regresion lineal para predecir calibracion usando tres metodos de regularizacion (ridge, lasso, elastic net), para predecir facetas.
Por otro lado, es importante para estas regularizaciones, elegir un lambda adecuado, el cual se suele elegir mediante algun metodo de cross validation. Usualmente, en los trabajos que revisamos, usan 10 fold corss validation, aunque por la poca cantidad de datos usaremos leave one out cross validation (LOOCV). Para eso crearemos nuestra propia funcion de LOOCV

Creo una funcion para realizar un LOOCV y elegir asi el valor de lambda optimo.

```{r}
#### 

LOOCV.fun.glmnet <- function(df, alpha.dado){
  ### Primero creo una secuencua de posibles lambdas, desde un modelo desechable
  
  # Guardo las observaciones de genero
  y <- df$Calibracion

  # ahora les saco genero y Participant (ya que no tienen que ir como predictores)
  df.lambda <- df %>% select(!c(Calibracion,Participant))

  # transformo en matrix
  df.lambda.m <- df.lambda %>% data.matrix()

  # Corro el modelo y obtengo el lambda
  modelo.lambda <- glmnet(df.lambda.m, y,
                            alpha = alpha.dado)
  posibles_lambdas <- modelo.lambda$lambda
  
  # creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
  matriz.errores <- matrix(data=NA,nrow= nrow(df),
                           ncol= length(posibles_lambdas))
  
  for (j in 1:nrow(df)){
    # datos de entrenamiento (todos menos 1)
    train <- df %>%
      filter(Participant != df$Participant[j])
    
    # agarro el dato que quedo para el testeo
    test  <- anti_join(df, train, by = 'Participant')
    
    # Guardo las observaciones de genero en entrenamiento y testeo (para entrenar el modelo y testearlo)
    y.train <- train$Calibracion
    y.test <- test$Calibracion
    
    # ahora les saco genero y Participant (ya que no tienen que ir como predictores)
    train <- train %>% select(!c(Calibracion,Participant))
    test <- test %>% select(!c(Calibracion,Participant))
    
    # transformo en matrix
    train.m <- train %>% data.matrix()
    test.m <- test %>% data.matrix()
    
    # corro el modelo con los datos de entrenamiento
    modelo <- glmnet(train.m, y.train,
                     alpha = alpha.dado, 
                     lambda = posibles_lambdas)
    
    # hago prediccion sobre la data de testeo
    matriz.errores[j,] <- modelo %>% predict(newx = test.m, type = "response")
  }
  
  # saco los root mean squared errors para cada lambda
  lambda.RMSE <- sqrt(((colSums(matriz.errores))**2)/nrow(df))
  
  list_return <- list("lambda.RMSE" = lambda.RMSE,
                      "lambdas" = posibles_lambdas)
  
  return(list_return)
  }

```

#Para facetas

Primero saco algunas variables que no van a ser necesarias.

```{r}

# saco las variables que no me interesan para esta parte de facetas
d.regre.regul.facetas <- d %>%
  select(!c(DomainNegativeAffect,
            DomainDetachment,
            DomainAntagonism,
            DomainDisinhibition,
            DomainPsychoticism,
            PC,
            ConfMean,
            ConfSD,
            ReacTimeMean_DiscTask,
            ReacTimeSD_DiscTask,
            ReacTimeMean_ConfTask,
            ReacTimeSD_ConfTask,
            mc,
            gender))
```

Ajusto una regresion con penalizacion Ridge. 

```{r, warning=F}
lambdaOptimo_facetas_ridge <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 0)

# lo ploteo 
plot(lambdaOptimo_facetas_ridge$lambda.RMSE,type ='l', log = 'x')

# tomo el lambda que tiene el menor error
index.lambda <- which.min(lambdaOptimo_facetas_ridge$lambda.RMSE)
lambda.optimo <- lambdaOptimo_facetas_ridge$lambdas[index.lambda]

### corro la regresion con el lambda optimo
y <- d.regre.regul.facetas$Calibracion

# ahora les saco genero y Participant (ya que no tienen que ir como predictores)
d.regre.regul.facetas.prepro <- d.regre.regul.facetas %>% select(!c(Calibracion,Participant))

# transformo en matrix
d.regre.regul.facetas.prepro.m <- d.regre.regul.facetas.prepro %>% data.matrix()

# Corro el modelo y obtengo el lambda
modelo.facetas_ridge <- glmnet(d.regre.regul.facetas.prepro.m, y,
                            alpha = 0, lambda = lambda.optimo)

coef(modelo.facetas_ridge)
```

Ajusto una regresion penalizacion de tipo Lasso

```{r}
lambdaOptimo_facetas_lasso <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 1)

# lo ploteo 
plot(lambdaOptimo_facetas_lasso$lambda.RMSE,type ='l', log = 'x')

# tomo el lambda que tiene el menor error
index.lambda <- which.min(lambdaOptimo_facetas_lasso$lambda.RMSE)
lambda.optimo <- lambdaOptimo_facetas_lasso$lambdas[index.lambda]

### corro la regresion con el lambda optimo
y <- d.regre.regul.facetas$Calibracion

# ahora les saco genero y Participant (ya que no tienen que ir como predictores)
d.regre.regul.facetas.prepro <- d.regre.regul.facetas %>% select(!c(Calibracion,Participant))

# transformo en matrix
d.regre.regul.facetas.prepro.m <- d.regre.regul.facetas.prepro %>% data.matrix()

# Corro el modelo y obtengo el lambda
modelo.facetas_lasso <- glmnet(d.regre.regul.facetas.prepro.m, y,
                            alpha = 1, lambda = lambda.optimo)

coef(modelo.facetas_lasso)
```

Ajusto una Elastic Net

```{r}
lambdaOptimo_facetas_ElasticNet <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 0.5)

# lo ploteo 
plot(lambdaOptimo_facetas_ElasticNet$lambda.RMSE,type ='l', log = 'x')

# tomo el lambda que tiene el menor error
index.lambda <- which.min(lambdaOptimo_facetas_ElasticNet$lambda.RMSE)
lambda.optimo <- lambdaOptimo_facetas_ElasticNet$lambdas[index.lambda]

### corro la regresion con el lambda optimo
y <- d.regre.regul.facetas$Calibracion

# ahora les saco genero y Participant (ya que no tienen que ir como predictores)
d.regre.regul.facetas.prepro <- d.regre.regul.facetas %>% select(!c(Calibracion,Participant))

# transformo en matrix
d.regre.regul.facetas.prepro.m <- d.regre.regul.facetas.prepro %>% data.matrix()

# Corro el modelo y obtengo el lambda
modelo.facetas_ElasticNet <- glmnet(d.regre.regul.facetas.prepro.m, y,
                            alpha = 0.5, lambda = lambda.optimo)

coef(modelo.facetas_ElasticNet)
```


# Dominios

X.1. Armar un vector llamado `columnas_de_dominio` con los nombres de las columnas que contienen las variables de interes para el caso de los dominios.

```{r}
columnas_de_dominio <- colnames(d)[str_detect(colnames(d), pattern = 'Domain')]
```

X.2. Generar un vector llamado `mejor_subconjunto` que contenga las formulas (como strings) de todos los posibles modelos que pueden generarse a partir de los 5 dominios a utilizar como covariadas. Las formulas tienen que ser strings que contenga la variable `gender` como variable de respuesta y los dominions como variables predictoras. 
La cantidad total de modelos posibles de manera general es $\sum_{k = 1}^n\frac{n!}{k!(n-k)!}$ que para este caso nos da 31 por lo que debemos obtener un vector con longitud 31. 

```{r}
mejor_subconjunto <- sapply(
  1:5, function(x) combn(columnas_de_dominio, x, simplify = F) %>% 
                 lapply(function(y) str_flatten(y, ' + ')) %>% 
                 unlist()
       )  %>% unlist() %>%
  sapply(
    function(x) paste0('Calibracion ~ ', x) 
  ) %>% unname()
```

X.3. Generar dos vectores llamados `formula_dominio_simple` y `formula_dominio_completa` que contenga las formulas con todos los predictores para los modelos univariados y para el modelo completo (multivariado), respectivamente. El vector `formula_dominio_simple` debe contener 5 modelos distintos, uno correspondiente a cada predictor y el vector `formula_dominio_completa` contiene un solo elemento que es el modelo con todas las covariadas agregadas. 

```{r}
formula_dominio_simple <- mejor_subconjunto[1:5]
formula_dominio_completa <- mejor_subconjunto[length(mejor_subconjunto)]
```

X.4. Generar una funcion llamada `bootstrap_logistico` que dada una string denominada `formula_string`, correspondiente a la formula de un modelo de regresion logistica, calcule la media, el percentil 2.5 y el percentil 97.5 de la distribucion bootstrap para los parametros de la regresion logistica utilizando la funcion `quantile` para los percentiles y usando el metodo de Bootstrap no parametrico con 10 mil repeticiones.

```{r}
bootstrap_logistico <- function(formula_string) {
  replicate(10000,
  lm(as.formula(formula_string),
      data = slice_sample(d, n =nrow(d), replace = T)
      )$coeff
  ) %>%
  apply(
    1, 
    FUN = function(x.row) c(media = mean(x.row), 
                            quantile(x.row, c(0.025,0.975))
                            )
  )
}
```

x.5. Utilizando los vectores `formula_dominio_simple` y `formula_dominio_completa`, itere por cada una de las formulas y en cada caso utilice la funcion `bootstrap_logistico` para calcular los parametros relevantes de la distribucion bootstrap de los coeficientes $\beta$ para cada predictor en el caso del modelo simple (con un solo predictor) y en el caso del modelo completo (con todos los predictores). El resultado debe ser un dataframe donde cada fila corresponda a un coeficiente, indicado por una columna llamada `coeficiente`, identificado por una columna llamada `tipo_modelo` que indica si el coeficiente proviene de un modelo univariado o multivariado y con tres columnas correspondientes a la media, el percentil 2.5 y el percentil 97.5 correspondiente a cada coeficiente. Guardar el dataframe resultante en una variable llamada `bootstrap_dominio`.

```{r}
set.seed(1234)
bootstrap_dominio <- sapply(
  c(formula_dominio_simple,formula_dominio_completa),
  bootstrap_logistico 
  ) %>% lapply(as.data.frame) %>% bind_rows() %>%
  mutate(
    tipo_modelo = rep(
      c('simple','completo'), c(3*5,3)
      )
         ) %>% 
  rownames_to_column(var = 'tipo_medicion') %>%
  mutate(tipo_medicion = str_replace(tipo_medicion, 'media\\d|\\...\\d+','')) %>% 
  pivot_longer(!starts_with('tipo'), names_to = 'coeficiente', values_to = 'estimacion') %>%
  drop_na() %>%
  filter(coeficiente != '(Intercept)') %>%
  pivot_wider(names_from = tipo_medicion, values_from = estimacion)
```

x.6. Graficar los valores de los coeficientes con sus respectivos intervalos para cada predictor indicando con un color si la estimacion proviene de los modelos simples o del modelo completo e indicando con una linea punteada el valor de 0 en el grafico. 

```{r}
bootstrap_dominio %>%
  ggplot(
    aes(x = coeficiente, y = media, color = tipo_modelo, ymin = `2.5%`, ymax = `97.5%`)
  ) +
  geom_point(
    position = position_dodge(.8)
  ) +
  geom_errorbar(
    width = .2,
    position = position_dodge(.8)
  ) +
  geom_hline(yintercept = 0) +
  coord_flip() +
  ylab('Estimacion') + xlab('Coeficientes') +
  scale_color_discrete('Tipo de Modelo', labels = c('Multiple', 'Simple'))
```

X.7. Interpretar los resultados

> En todos los casos los intervalos de confianza del 95% de las variables predictoras, ya sea para el modelo univariado o multivariado, alcanzan el valor del 0 lo cual indica que no podemos concluir que la pendiente de alguna de las covariadas sea distinta de 0 dados los datos. 

x.8. Encontrar el modelo que tiene un mejor desempeÃ±o predictivo en el conjunto de testeo. Para eso utilizar el vector `mejor_subconjunto` para iterar por cada modelo evaluando su desempeÃ±o predictivo segun la media de los errores cuadraticos utilizando el procedimiento leave-one-out para elegir. Guardar el error cuadratrico 

```{r}
predicciones_mejor_subconjunto <- pmap(
  expand.grid(1:nrow(d), mejor_subconjunto) %>% mutate(Var2 = as.character(Var2)),
  ~ lm(
      as.formula(.y), data = d, subset = -.x
    ) %>% predict(d[.x, ], type = 'response') - d[.x, 'Calibracion']
) %>%
  unlist() %>% mutate(expand.grid(1:nrow(d), mejor_subconjunto), pred = .,
                      error = pred - d[Var1, 'Calibracion'],
                      error_cuadratico = error^2) %>%
  rename(Modelo = Var2)
```

X.9. Graficar el desempeÃ±o predictivo de cada uno de los modelos. 

```{r}
ggplot(predicciones_mejor_subconjunto,
  aes(x = Modelo, y = error_cuadratico)
) +
  geom_boxplot() +
  coord_flip() +
  theme(axis.text.y = element_text(size = 5))
```

x.10. Interpretar los resultados

> No se observan diferencias en la capacidad predictiva de los distintos modelos sobre el conjunto de testeo segun el metodo de validacion cruzada leave-one-out. Por lo tanto dada la varianza en los resultados no seria prudente elegir el valor que minimiza la media de los errores cuadraticos empiricos considerando que no habria suficiente evidencia para concluir que los dominios de personalidad predicen la calibracion. 

