---
title: "TrabajoFinal"
author: "Alejandro Ramos Usaj, Agostina Sacson y Iair Embon"
date: '2022-07-25'
output: html_document
---

```{r}
library(tidyverse)
```



Cargo los datos
```{r}
# voy a la carpeta del proyecto
root <- rprojroot::is_rstudio_project
basename(getwd())

# load the function to get the df list
source(root$find_file("Entrega_Final/df_total_filtered.Rda")) ## me tira un error aca, no se poque lo voy a esta resolviendo

load("~/Documents/Investigación/Curso.DS.R/Entrega_Final/df_total_filtered.Rda")
```

Lo exploro
```{r}
str(df_total)
```

Veo la cantidad de participantes
```{r}
print(length(unique(df_total$Participant)))
```


Exploro la variable de interes (Confianza)
```{r}
hist(df_total$confidence_key)
```

Convierto el df por trials en un df por participantes
```{r}
library(dplyr)

d <- df_total %>%
  select(!c(RelyOn,
            Problems,
            ConfKey1,
            ConfKey2,
            ConfKey3,
            ConfKey4,
            discrimination_is_correct,
            confidence_key,
            trials,
            PointDifference,
            ReacTime_DiscTask,
            ReacTime_ConfTask)) %>%
  distinct(Participant,.keep_all = TRUE)
```

Exploro la otra variable de interes (metacognicion)
```{r}
plot(density(d$mc))
```


# Validacion Cruzada y Test Set

Para comparar el desempeño de distintos algoritmos vamos a separar los datos en un conjunto de entrenamiento y un conjunto de testeo. Para eso vamos a reservar el 10% de los datos para testeo y usar el 90% restante para entrenamiento. 

1. Defina una variable llamada `train_prop` y asigne el valor acorde a una proporcion del 90% de los casos. 
2. Defina un vector llamado `train_indices` con los indices que marcan aquellas filas que van a integrar nuestro conjunto de entrenamiento con una proporcion definida por `train_prop`.  

```{r}
train_prop <- 0.9
train_indices <- sample(
  nrow(df_total),
  round(nrow(df_total)*train_prop)
)
```

Con ese vector ya definido separamos los dos datasets. 

```{r}
train_set <- df_total[train_indices, ]
test_set <- df_total[-train_indices, ]
```

A su vez necesitamos implementar un metodo de validacion cruzada para el ajuste de hiperparametros en los distintos algoritmos. Dado el volumen de datos consideramos que lo mas optimo es implementar una validacion cruzada de tipo leave-one-out asi incrementamos la cantidad de datos que podemos usar para entrenar los modelos. Para esto necesitamos crear una funcion que pueda ser generalizable para cualquier cantidad de hiperparametros (en tanto dimensiones) y tambien para distintas metricas. 

Lo que queremos entonces es una funcion llamada `loocv_fun` 

```{r}
loocv_fun <- function(datos, lista_hiperparametros, metodo){
  loocv_sample <- function(indice, ...){
    train_loocv <- datos[-indice, ]
    test_loocv <- datos[indice, ]
    metodo(train_loocv, test_loocv, ...)
  }
  
  
  #Itero por los hiperparametros
  pmap(
    lista_hiperparametros,
    metodo,
    datos = datos
  )
}

lapply(
      1:nrow(datos),
      function(loocv_ind) metodo(datos[-loocv_ind, ])
    )
```



```{r}
sapply(
      1:nrow(iris),
      function(loocv_ind) mean(iris$Sepal.Length[-loocv_ind])
    )
```

