---
title: "TrabajoFinal"
author: "Alejandro Ramos Usaj, Agostina Sacson y Iair Embon"
date: '2022-07-25'
output: html_document
---

```{r}
library(tidyverse)
```



Cargo los datos
```{r}
# voy a la carpeta del proyecto
root <- rprojroot::is_rstudio_project
basename(getwd())

# load the function to get the df list
source(root$find_file("Entrega_Final/df_total_filtered.Rda")) ## me tira un error aca, no se poque lo voy a esta resolviendo

load("~/Documents/Investigación/Curso.DS.R/Entrega_Final/df_total_filtered.Rda")
```

Lo exploro
```{r}
str(df_total)
```

Veo la cantidad de participantes
```{r}
print(length(unique(df_total$Participant)))
```


Exploro la variable de interes (Confianza)
```{r}
hist(df_total$confidence_key)
```

Convierto el df por trials en un df por participantes
```{r}
library(dplyr)

d <- df_total %>%
  select(!c(RelyOn,
            Problems,
            ConfKey1,
            ConfKey2,
            ConfKey3,
            ConfKey4,
            discrimination_is_correct,
            confidence_key,
            trials,
            PointDifference,
            ReacTime_DiscTask,
            ReacTime_ConfTask)) %>%
  distinct(Participant,.keep_all = TRUE)
```

Exploro la otra variable de interes (metacognicion)
```{r}
plot(density(d$mc))
```


# Validacion Cruzada y Test Set

Para comparar el desempeño de distintos algoritmos vamos a separar los datos en un conjunto de entrenamiento y un conjunto de testeo. Para eso vamos a reservar el 10% de los datos para testeo y usar el 90% restante para entrenamiento. 

```{r}
train_indices <- sample(
  nrow(df_total),
  round(nrow(df_total)*0.9)
)
train_set <- df_total[train_indices, ]
test_set <- df_total[-train_indices, ]
```

A su vez necesitamos implementar un metodo de validacion cruzada para el ajuste de hiperparametros en los distintos algoritmos. Dado el volumen de datos consideramos que lo mas optimo es implementar una validacion cruzada de tipo leave-one-out. Para esto necesitamos crear una funcion que pueda ser generalizable para cualquier cantidad de hiperparametros (en tanto dimensiones) y tambien para distintas metricas.

```{r}
loocv_fun <- function(datos, lista_hiperparametros, metodo){
  loocv_sample <- function(indice, ...){
    train_loocv <- datos[-indice, ]
    test_loocv <- datos[indice, ]
    metodo(train_loocv, test_loocv, ...)
  }
  
  
  #Itero por los hiperparametros
  pmap(
    lista_hiperparametros,
    metodo,
    datos = datos
  )
}

lapply(
      1:nrow(datos),
      function(loocv_ind) metodo(datos[-loocv_ind, ])
    )
```



```{r}
sapply(
      1:nrow(iris),
      function(loocv_ind) mean(iris$Sepal.Length[-loocv_ind])
    )
```

