# creo una secuencua de posibles lambdas
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
for (j in 1:nrow(d.regre.regul.facetas)){
# agrego una variable ID (participantes la saque para correr el modelo)
d.regre.regul.facetas <- d.regre.regul.facetas %>%
mutate(id = row_number())
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(id != d.regre.regul.facetas$id[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'id')
# Guardo las observaciones de overconfidence en entrenamiento y testeo(para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
# ahora les saco overconfidence (ya que no tiene que ir como predictor)
train <- train %>%
select(!overconfidence)
test <- test %>%
select(!overconfidence)
# transformo en matrix
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
# corro el modelo con los datos de entrenamiento
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
# tomo los lambdas generados por el modelo (los mismo que le puse, pero tomo su orden)
lambdas <- modelo$lambda
# Creo la variable en la que voy a guardar el error para cada lambda
error <- rep(NaN, length(nrow(d.regre.regul.facetas)))
for (i in 1:length(lambdas)) {
# obtengo el modelo que genera cada lambda
lambda_i <- coef(modelo, s = lambdas[i])
# hago prediccion sobre la data de testeo
probabilities <- lambda_i %>% predict(new = test.m, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy
caso.observado <- y.test
error[i] <- mean(caso.prediccion == caso.observado) # revisar que sacar aca
}
}
library(tidyverse)
# voy a la carpeta del proyecto
root <- rprojroot::is_rstudio_project
basename(getwd())
# load the function to get the df list
# source(root$find_file("Entrega_Final/df_total_filtered.Rda")) ## me tira un error aca, no se poque lo voy a esta resolviendo
load("./df_total_filtered.Rda")
str(df_total)
print(length(unique(df_total$Participant)))
hist(df_total$confidence_key)
library(tidyverse)
d <- df_total %>%
select(!c(RelyOn,
Problems,
ConfKey1,
ConfKey2,
ConfKey3,
ConfKey4,
discrimination_is_correct,
confidence_key,
trials,
PointDifference,
ReacTime_DiscTask,
ReacTime_ConfTask)) %>%
distinct(Participant,.keep_all = TRUE)
plot(density(d$mc))
d %>%  ## no me estarian saliendo las leyendas, despues reviso
select(ConfMean,PC,Participant) %>%
# normalizo el promedio de confianza por sujeto para que este en una escala similar a la del desempeno
mutate(ConfMean = ConfMean/4, nr = row_number()) %>%
arrange(ConfMean) %>%
mutate(nr = row_number()) %>%
ggplot(aes(x= nr)) +
geom_point(aes(y=ConfMean), size = 2, color = "black") +
geom_point(aes(y=PC), size = 2, color = "grey")+
scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
scale_y_continuous(expand = expansion(mult = c(0.01, 0.1))) +
theme(axis.line = element_line(colour = "black"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
plot.margin = margin(1, 1,1, 1, "cm"),
legend.text =  element_text(size = 25),
legend.position = c(0.7, 0.2),
legend.background = element_blank(),
legend.key = element_blank(),
legend.title = element_blank(),
panel.background = element_blank(),
axis.text.x = element_text(size = 30),
axis.text.y = element_text(size = 30),
axis.title.x = element_blank(),
axis.title.y = element_blank())
d <- d %>%
mutate(ConfMean.norm = ConfMean/4,
overconfidence = ifelse(ConfMean.norm > 0.72, 1, 0))
# veo cuantos hay
d %>%
summarise(n= sum(overconfidence))
# hay 81 participantes con alta confianza, lo que nos da 143 con baja confianza
d$gender <- ifelse(d$gender == "Masculino",1,0)
model <- glm(overconfidence ~ DomainAntagonism +
DomainDetachment +
DomainDisinhibition +
DomainNegativeAffect +
DomainPsychoticism,
family=binomial(link='logit'),
data=d)
summary(model)
model2 <- glm(overconfidence ~ DomainAntagonism +
DomainDetachment +
DomainDisinhibition +
DomainNegativeAffect +
DomainPsychoticism +
gender +
age,
family=binomial(link='logit'),
data=d)
summary(model2)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
# creo una secuencua de posibles lambdas
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
j <- 1
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# saco las variables que no me interesan para esta parte de facetas
d.regre.regul.facetas <- d %>%
select(!c(DomainNegativeAffect,
DomainDetachment,
DomainAntagonism,
DomainDisinhibition,
DomainPsychoticism,
PC,
ConfMean,
ConfSD,
ReacTimeMean_DiscTask,
ReacTimeSD_DiscTask,
ReacTimeMean_ConfTask,
ReacTimeSD_ConfTask,
mc,
ConfMean.norm))
d_mat <- d.regre.regul.facetas %>%
data.matrix()
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'Participant')
entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
train <- train %>%
select(!c(overconfidence,Participant))
test <- test %>%
select(!c(overconfidence,Participant))
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
library(glmnet)
# corro el modelo con los datos de entrenamiento
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
# tomo los lambdas generados por el modelo (los mismo que le puse, pero tomo su orden)
lambdas <- modelo$lambda
lambdas
# Creo el vector en la que voy a guardar el error del modelo para n-j para cada lambda
error <- rep(NaN, length(nrow(d.regre.regul.facetas)))
# Creo el vector en la que voy a guardar el error del modelo para n-j para cada lambda
error <- rep(NaN, length(lambdas))
i <- 1
lambda_i <- coef(modelo, s = lambdas[i])
lambda_i
probabilities <- lambda_i %>% predict(new = test.m, type = "response")
test.m
lambda_i
# hago prediccion sobre la data de testeo
probabilities <- lambda_i %>% predict(newx = test.m, type = "response")
lambda_i
predict(object = modelo, newx = test.m, type = "response")
# hago prediccion sobre la data de testeo
probabilities <- modelo %>% predict(newx = test.m, type = "response")
probabilities
predicted.classes
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
predicted.classes
error[i] <- mean(caso.prediccion == caso.observado)
caso.observado <- y.test
caso.observado
error[i] <- mean(caso.prediccion == caso.observado)
y.test
predicted.classes == y.test
mean(predicted.classes == y.test)
probabilities <- modelo %>% predict(newx = test.m, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
predicted.classes == y.test
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
matriz.errores[j,]
matriz.errores[2,]
# creo una secuencua de posibles lambdas
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
for (j in 1:nrow(d.regre.regul.facetas)){
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
# ahora les saco overconfidence y Participant (ya que no tienen que ir como predictores)
train <- train %>%
select(!c(overconfidence,Participant))
test <- test %>%
select(!c(overconfidence,Participant))
# transformo en matrix
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
# corro el modelo con los datos de entrenamiento
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
# hago prediccion sobre la data de testeo
probabilities <- modelo %>% predict(newx = test.m, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
}
matriz.errores[1,1]
matriz.errores[1,]
matriz.errores[2,]
matriz.errores[50,]
matriz.errores[55,]
matriz.errores[100,]
rowSums(matriz.errores)
nrow(d.regre.regul.facetas)
# saco los root mean squared errors para cada lambda
lambda.RMSE <- sqrt(((rowSums(matriz.errores))**2)/nrow(d.regre.regul.facetas))
lambda.RMSE
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
y.train
y.test
train
j <- 1
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'Participant')
y.train <- train$overconfidence
y.test <- test$overconfidence
y.train
y.test
train <- train %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
test.m
test
test
test
test <- test %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
probabilities <- modelo %>% predict(newx = test.m, type = "response")
probabilities
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
predicted.classes
matriz.errores[j,] <- predicted.classes == y.test
matriz.errores
# creo una secuencua de posibles lambdas
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
for (j in 1:nrow(d.regre.regul.facetas)){
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
# ahora les saco overconfidence y Participant (ya que no tienen que ir como predictores)
train <- train %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
# transformo en matrix
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
# corro el modelo con los datos de entrenamiento
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
# hago prediccion sobre la data de testeo
probabilities <- modelo %>% predict(newx = test.m, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
}
# saco los root mean squared errors para cada lambda
lambda.RMSE <- sqrt(((rowSums(matriz.errores))**2)/nrow(d.regre.regul.facetas))
lambda.RMSE <- sqrt(((rowSums(matriz.errores))**2)/nrow(d.regre.regul.facetas))
lambda.RMSE
j <- 1
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
# ahora les saco overconfidence y Participant (ya que no tienen que ir como predictores)
train <- train %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
# transformo en matrix
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
cvfit <- cv.glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
plot(cvfit)
cvfit$lambda.min
coef(cvfit, s = "lambda.min")
cvfit$lambda.min
plot(cvfit)
log(20)
lambda.RMSE
which(lambda.RMSE)
which(lambda.RMSE.min)
which(min(lambda.RMSE))
which(lambda.RMSE,min(lambda.RMSE))
which.min(lambda.RMSE)
LOOCV.fun.glmnet <- function(df){
# creo una secuencua de posibles lambdas
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(df),
ncol= length(posibles_lambdas))
for (j in 1:nrow(df)){
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != df$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(df, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
# ahora les saco overconfidence y Participant (ya que no tienen que ir como predictores)
train <- train %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
# transformo en matrix
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
# corro el modelo con los datos de entrenamiento
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
# hago prediccion sobre la data de testeo
probabilities <- modelo %>% predict(newx = test.m, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
}
# saco los root mean squared errors para cada lambda
lambda.RMSE <- sqrt(((rowSums(matriz.errores))**2)/nrow(df))
return(lambda.RMSE)
}
library(glmnet)
d.regre.regul.facetas
# saco las variables que no me interesan para esta parte de facetas
d.regre.regul.facetas <- d %>%
select(!c(DomainNegativeAffect,
DomainDetachment,
DomainAntagonism,
DomainDisinhibition,
DomainPsychoticism,
PC,
ConfMean,
ConfSD,
ReacTimeMean_DiscTask,
ReacTimeSD_DiscTask,
ReacTimeMean_ConfTask,
ReacTimeSD_ConfTask,
mc,
ConfMean.norm))
lambdaOptimo_facetas <- LOOCV.fun.glmnet(d.regre.regul.facetas)
lambdaOptimo_facetas <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 0)
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
LOOCV.fun.glmnet <- function(df, alpha.dado){
# creo una secuencua de posibles lambdas
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(df),
ncol= length(posibles_lambdas))
for (j in 1:nrow(df)){
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != df$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(df, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
# ahora les saco overconfidence y Participant (ya que no tienen que ir como predictores)
train <- train %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
# transformo en matrix
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
# corro el modelo con los datos de entrenamiento
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = alpha.dado , lambda = posibles_lambdas)
# hago prediccion sobre la data de testeo
probabilities <- modelo %>% predict(newx = test.m, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
}
# saco los root mean squared errors para cada lambda
lambda.RMSE <- sqrt(((rowSums(matriz.errores))**2)/nrow(df))
return(lambda.RMSE)
}
lambdaOptimo_facetas <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 0)
lambdaOptimo_facetas
lambdaOptimo_facetas_ridge <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 0)
lambdaOptimo_facetas_lasso <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 1)
lambdaOptimo_facetas_elastic
lambdaOptimo_facetas_elastic <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 0.5)
lambdaOptimo_facetas_elastic
lambdaOptimo_facetas_lasso
load("D:/Windows/Descargas/Curso.DS.R/Entrega_Final/df_total_filtered.Rda")
str(df_total)
library(tidyverse)
# voy a la carpeta del proyecto
root <- rprojroot::is_rstudio_project
basename(getwd())
# load the function to get the df list
# source(root$find_file("Entrega_Final/df_total_filtered.Rda")) ## me tira un error aca, no se poque lo voy a esta resolviendo
load("./df_total_filtered.Rda")
str(df_total)
print(length(unique(df_total$Participant)))
hist(df_total$confidence_key)
d <- df_total %>%
select(!c(RelyOn,
Problems,
ConfKey1,
ConfKey2,
ConfKey3,
ConfKey4,
discrimination_is_correct,
confidence_key,
trials,
PointDifference,
ReacTime_DiscTask,
ReacTime_ConfTask)) %>%
distinct(Participant,.keep_all = TRUE)
plot(density(d$mc))
d %>%  ## no me estarian saliendo las leyendas, despues reviso
select(ConfMean,PC,Participant) %>%
# normalizo el promedio de confianza por sujeto para que este en una escala similar a la del desempeno
mutate(ConfMean = ConfMean/4, nr = row_number()) %>%
arrange(ConfMean) %>%
mutate(nr = row_number()) %>%
ggplot(aes(x= nr)) +
geom_point(aes(y=ConfMean), size = 2, color = "black") +
geom_point(aes(y=PC), size = 2, color = "grey")+
scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
scale_y_continuous(expand = expansion(mult = c(0.01, 0.1))) +
theme(axis.line = element_line(colour = "black"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
plot.margin = margin(1, 1,1, 1, "cm"),
legend.text =  element_text(size = 25),
legend.position = c(0.7, 0.2),
legend.background = element_blank(),
legend.key = element_blank(),
legend.title = element_blank(),
panel.background = element_blank(),
axis.text.x = element_text(size = 30),
axis.text.y = element_text(size = 30),
axis.title.x = element_blank(),
axis.title.y = element_blank())
