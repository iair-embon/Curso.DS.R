probabilities <- lambda_i %>% predict(new = test.m, type = "response")
test.m
lambda_i
# hago prediccion sobre la data de testeo
probabilities <- lambda_i %>% predict(newx = test.m, type = "response")
lambda_i
predict(object = modelo, newx = test.m, type = "response")
# hago prediccion sobre la data de testeo
probabilities <- modelo %>% predict(newx = test.m, type = "response")
probabilities
predicted.classes
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
predicted.classes
error[i] <- mean(caso.prediccion == caso.observado)
caso.observado <- y.test
caso.observado
error[i] <- mean(caso.prediccion == caso.observado)
y.test
predicted.classes == y.test
mean(predicted.classes == y.test)
probabilities <- modelo %>% predict(newx = test.m, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
predicted.classes == y.test
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
matriz.errores[j,]
matriz.errores[2,]
# creo una secuencua de posibles lambdas
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
for (j in 1:nrow(d.regre.regul.facetas)){
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
# ahora les saco overconfidence y Participant (ya que no tienen que ir como predictores)
train <- train %>%
select(!c(overconfidence,Participant))
test <- test %>%
select(!c(overconfidence,Participant))
# transformo en matrix
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
# corro el modelo con los datos de entrenamiento
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
# hago prediccion sobre la data de testeo
probabilities <- modelo %>% predict(newx = test.m, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
}
matriz.errores[1,1]
matriz.errores[1,]
matriz.errores[2,]
matriz.errores[50,]
matriz.errores[55,]
matriz.errores[100,]
rowSums(matriz.errores)
nrow(d.regre.regul.facetas)
# saco los root mean squared errors para cada lambda
lambda.RMSE <- sqrt(((rowSums(matriz.errores))**2)/nrow(d.regre.regul.facetas))
lambda.RMSE
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
y.train
y.test
train
j <- 1
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'Participant')
y.train <- train$overconfidence
y.test <- test$overconfidence
y.train
y.test
train <- train %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
test.m
test
test
test
test <- test %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
probabilities <- modelo %>% predict(newx = test.m, type = "response")
probabilities
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
predicted.classes
matriz.errores[j,] <- predicted.classes == y.test
matriz.errores
# creo una secuencua de posibles lambdas
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
for (j in 1:nrow(d.regre.regul.facetas)){
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
# ahora les saco overconfidence y Participant (ya que no tienen que ir como predictores)
train <- train %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
# transformo en matrix
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
# corro el modelo con los datos de entrenamiento
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
# hago prediccion sobre la data de testeo
probabilities <- modelo %>% predict(newx = test.m, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
}
# saco los root mean squared errors para cada lambda
lambda.RMSE <- sqrt(((rowSums(matriz.errores))**2)/nrow(d.regre.regul.facetas))
lambda.RMSE <- sqrt(((rowSums(matriz.errores))**2)/nrow(d.regre.regul.facetas))
lambda.RMSE
j <- 1
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(d.regre.regul.facetas),
ncol= length(posibles_lambdas))
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != d.regre.regul.facetas$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(d.regre.regul.facetas, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
# ahora les saco overconfidence y Participant (ya que no tienen que ir como predictores)
train <- train %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
# transformo en matrix
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
cvfit <- cv.glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
plot(cvfit)
cvfit$lambda.min
coef(cvfit, s = "lambda.min")
cvfit$lambda.min
plot(cvfit)
log(20)
lambda.RMSE
which(lambda.RMSE)
which(lambda.RMSE.min)
which(min(lambda.RMSE))
which(lambda.RMSE,min(lambda.RMSE))
which.min(lambda.RMSE)
LOOCV.fun.glmnet <- function(df){
# creo una secuencua de posibles lambdas
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(df),
ncol= length(posibles_lambdas))
for (j in 1:nrow(df)){
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != df$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(df, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
# ahora les saco overconfidence y Participant (ya que no tienen que ir como predictores)
train <- train %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
# transformo en matrix
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
# corro el modelo con los datos de entrenamiento
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = 0, lambda = posibles_lambdas)
# hago prediccion sobre la data de testeo
probabilities <- modelo %>% predict(newx = test.m, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
}
# saco los root mean squared errors para cada lambda
lambda.RMSE <- sqrt(((rowSums(matriz.errores))**2)/nrow(df))
return(lambda.RMSE)
}
library(glmnet)
d.regre.regul.facetas
# saco las variables que no me interesan para esta parte de facetas
d.regre.regul.facetas <- d %>%
select(!c(DomainNegativeAffect,
DomainDetachment,
DomainAntagonism,
DomainDisinhibition,
DomainPsychoticism,
PC,
ConfMean,
ConfSD,
ReacTimeMean_DiscTask,
ReacTimeSD_DiscTask,
ReacTimeMean_ConfTask,
ReacTimeSD_ConfTask,
mc,
ConfMean.norm))
lambdaOptimo_facetas <- LOOCV.fun.glmnet(d.regre.regul.facetas)
lambdaOptimo_facetas <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 0)
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
LOOCV.fun.glmnet <- function(df, alpha.dado){
# creo una secuencua de posibles lambdas
posibles_lambdas <- seq(0.01, 20 , length.out = 300)
# creo una matriz en donde se van a ir guardando los errores de cada lambda y cada vez que hago LOOCV
matriz.errores <- matrix(data=NA,nrow= nrow(df),
ncol= length(posibles_lambdas))
for (j in 1:nrow(df)){
# datos de entrenamiento (todos menos 1)
train <- d.regre.regul.facetas %>%
filter(Participant != df$Participant[j])
# agarro el dato que quedo para el testeo
test  <- anti_join(df, train, by = 'Participant')
# Guardo las observaciones de overconfidence en entrenamiento y testeo (para entrenar el modelo y testearlo)
y.train <- train$overconfidence
y.test <- test$overconfidence
# ahora les saco overconfidence y Participant (ya que no tienen que ir como predictores)
train <- train %>% select(!c(overconfidence,Participant))
test <- test %>% select(!c(overconfidence,Participant))
# transformo en matrix
train.m <- train %>% data.matrix()
test.m <- test %>% data.matrix()
# corro el modelo con los datos de entrenamiento
modelo <- glmnet(train.m, y.train, family = "binomial",
alpha = alpha.dado , lambda = posibles_lambdas)
# hago prediccion sobre la data de testeo
probabilities <- modelo %>% predict(newx = test.m, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy
matriz.errores[j,] <- predicted.classes == y.test
}
# saco los root mean squared errors para cada lambda
lambda.RMSE <- sqrt(((rowSums(matriz.errores))**2)/nrow(df))
return(lambda.RMSE)
}
lambdaOptimo_facetas <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 0)
lambdaOptimo_facetas
lambdaOptimo_facetas_ridge <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 0)
lambdaOptimo_facetas_lasso <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 1)
lambdaOptimo_facetas_elastic
lambdaOptimo_facetas_elastic <- LOOCV.fun.glmnet(d.regre.regul.facetas, alpha.dado = 0.5)
lambdaOptimo_facetas_elastic
lambdaOptimo_facetas_lasso
load("D:/Windows/Descargas/Curso.DS.R/Entrega_Final/df_total_filtered.Rda")
str(df_total)
library(tidyverse)
# voy a la carpeta del proyecto
root <- rprojroot::is_rstudio_project
basename(getwd())
# load the function to get the df list
# source(root$find_file("Entrega_Final/df_total_filtered.Rda")) ## me tira un error aca, no se poque lo voy a esta resolviendo
load("./df_total_filtered.Rda")
str(df_total)
print(length(unique(df_total$Participant)))
hist(df_total$confidence_key)
d <- df_total %>%
select(!c(RelyOn,
Problems,
ConfKey1,
ConfKey2,
ConfKey3,
ConfKey4,
discrimination_is_correct,
confidence_key,
trials,
PointDifference,
ReacTime_DiscTask,
ReacTime_ConfTask)) %>%
distinct(Participant,.keep_all = TRUE)
plot(density(d$mc))
d %>%  ## no me estarian saliendo las leyendas, despues reviso
select(ConfMean,PC,Participant) %>%
# normalizo el promedio de confianza por sujeto para que este en una escala similar a la del desempeno
mutate(ConfMean = ConfMean/4, nr = row_number()) %>%
arrange(ConfMean) %>%
mutate(nr = row_number()) %>%
ggplot(aes(x= nr)) +
geom_point(aes(y=ConfMean), size = 2, color = "black") +
geom_point(aes(y=PC), size = 2, color = "grey")+
scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
scale_y_continuous(expand = expansion(mult = c(0.01, 0.1))) +
theme(axis.line = element_line(colour = "black"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
plot.margin = margin(1, 1,1, 1, "cm"),
legend.text =  element_text(size = 25),
legend.position = c(0.7, 0.2),
legend.background = element_blank(),
legend.key = element_blank(),
legend.title = element_blank(),
panel.background = element_blank(),
axis.text.x = element_text(size = 30),
axis.text.y = element_text(size = 30),
axis.title.x = element_blank(),
axis.title.y = element_blank())
library(tidyverse)
library(glmnet)
# voy a la carpeta del proyecto
root <- rprojroot::is_rstudio_project
basename(getwd())
# load the function to get the df list
load("./df_total_filtered.Rda")
df_calibracion <- df_total %>%
group_by(Participant, confidence_key) %>%
summarise(prop_correcta = mean(discrimination_is_correct)) %>%
rename(ConfidenceKey = confidence_key)
df_calibracion.calculada <- df_total %>%
pivot_longer(cols = starts_with("ConfKey")) %>%
distinct(Participant, name, .keep_all = TRUE) %>%
select(Participant, name, value) %>%
mutate(ConfidenceKey = as.integer(str_extract(name, "\\d"))) %>%
left_join(df_calibracion) %>%
drop_na() %>%
mutate(ConfidenceKey = (ConfidenceKey+2)/6) %>%
group_by(Participant) %>%
mutate(Conf.norm = value / sum(value)) %>%
mutate( preCalibracion = value*(prop_correcta - ConfidenceKey)**2) %>%
summarise(Calibracion  = mean(preCalibracion))
d <- df_total %>%
select(!c(RelyOn,
Problems,
ConfKey1,
ConfKey2,
ConfKey3,
ConfKey4,
discrimination_is_correct,
confidence_key,
trials,
PointDifference,
ReacTime_DiscTask,
ReacTime_ConfTask)) %>%
distinct(Participant,.keep_all = TRUE)
# le agrego la variable calibracion
d <- d %>%
mutate(Calibracion = df_calibracion.calculada$Calibracion)
d %>%  ## no me estarian saliendo las leyendas, despues reviso
select(ConfMean,PC,Participant) %>%
# normalizo el promedio de confianza por sujeto para que este en una escala similar a la del desempeno
mutate(ConfMean = ConfMean/4, nr = row_number()) %>%
arrange(ConfMean) %>%
mutate(nr = row_number()) %>%
ggplot(aes(x= nr)) +
geom_point(aes(y=ConfMean), size = 2, color = "black") +
geom_point(aes(y=PC), size = 2, color = "grey")+
scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
scale_y_continuous(expand = expansion(mult = c(0.01, 0.1))) +
theme(axis.line = element_line(colour = "black"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
plot.margin = margin(1, 1,1, 1, "cm"),
legend.text =  element_text(size = 25),
legend.position = c(0.7, 0.2),
legend.background = element_blank(),
legend.key = element_blank(),
legend.title = element_blank(),
panel.background = element_blank(),
axis.text.x = element_text(size = 30),
axis.text.y = element_text(size = 30),
axis.title.x = element_blank(),
axis.title.y = element_blank())
hist(d$Calibracion)
model <- lm(Calibracion ~ DomainAntagonism +
DomainDetachment +
DomainDisinhibition +
DomainNegativeAffect +
DomainPsychoticism,
data=d)
summary(model)
model2 <- lm(Calibracion ~ DomainAntagonism +
DomainDetachment +
DomainDisinhibition +
DomainNegativeAffect +
DomainPsychoticism +
gender +
age,
data=d)
summary(model2)
str(d)
# saco las variables que no necesito
d %>%
select(!c(DomainNegativeAffect,
DomainDetachment,
DomainAntagonism,
DomainDisinhibition,
DomainPsychoticism,
PC,
ConfMean,
ConfSD,
ReacTimeMean_DiscTask,
ReacTimeSD_DiscTask,
ReacTimeMean_ConfTask,
ReacTimeSD_ConfTask,
mc,
gender)) %>%
as.matrix() %>%
heatmap()
library(corrplot)
install.packages("corrplot")
# saco las variables que no necesito
d.cor <- d %>%
select(!c(DomainNegativeAffect,
DomainDetachment,
DomainAntagonism,
DomainDisinhibition,
DomainPsychoticism,
PC,
ConfMean,
ConfSD,
ReacTimeMean_DiscTask,
ReacTimeSD_DiscTask,
ReacTimeMean_ConfTask,
ReacTimeSD_ConfTask,
mc,
gender)) %>%
as.matrix() %>%
heatmap()
# saco las variables que no necesito
d.cor <- d %>%
select(!c(DomainNegativeAffect,
DomainDetachment,
DomainAntagonism,
DomainDisinhibition,
DomainPsychoticism,
PC,
ConfMean,
ConfSD,
ReacTimeMean_DiscTask,
ReacTimeSD_DiscTask,
ReacTimeMean_ConfTask,
ReacTimeSD_ConfTask,
mc,
gender))
library(corrplot)
corrplot(cor(d.cor), method = "ellipse")
corrplot(cor(d.cor), method = "ellipse")
corrplot(cor(d.cor), type="upper", method = "ellipse")
corrplot(cor(d.cor), type="upper", method = "ellipse")
corrplot(cor(d.cor), type="upper", method = "ellipse")
cor(d.cor)
str(d.cor)
# saco las variables que no necesito
d.cor <- d %>%
select(!c(DomainNegativeAffect,
DomainDetachment,
DomainAntagonism,
DomainDisinhibition,
DomainPsychoticism,
PC,
ConfMean,
ConfSD,
ReacTimeMean_DiscTask,
ReacTimeSD_DiscTask,
ReacTimeMean_ConfTask,
ReacTimeSD_ConfTask,
mc,
gender,
Participant,
age,
Calibracion))
library(corrplot)
corrplot(cor(d.cor), type="upper", method = "ellipse")
cor(d.cor)
sum(cor(d.cor) > 0.3)
sum(cor(d.cor) > 0.1)
sum(cor(d.cor) > 0)
sum(cor(d.cor) > 0.5)
sum(cor(d.cor) > 0.7)
sum(cor(d.cor) > 0.5)
sum(cor(d.cor) > 0.6)
25*25
corrplot(cor(d.cor), type="upper", method = "ellipse", tl.srt=45)
